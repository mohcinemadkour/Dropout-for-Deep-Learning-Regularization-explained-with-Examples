# Tutorial: Dropout as Regularization and Bayesian Approximation

This tutorial aims to give readers a complete view of dropout, which includes the implementation of dropout (in [PyTorch](https://pytorch.org/)), how to use dropout and why dropout is useful. Basically, dropout can (1) reduce overfitting (so test results will be better) and (2) provide model uncertainty.

Please view my tutorial [here](https://medium.com/@mohcine.madkour/dropout-for-deep-learning-regularization-explained-with-examples-dee81f0de35a).


## References

[1] [Improving neural networks by preventing co-adaptation of feature detectors](https://arxiv.org/pdf/1207.0580.pdf), G. E. Hinton, et al., 2012  
[2] [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](https://arxiv.org/pdf/1506.02142.pdf), Y. Gal, and Z. Ghahramani, 2016  
[3] [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf), N. Srivastava, et al., 2014
